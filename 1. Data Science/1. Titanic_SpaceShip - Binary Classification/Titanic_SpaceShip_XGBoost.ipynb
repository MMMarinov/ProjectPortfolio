{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing Libraries & titanic_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanic_data_clean Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Misc\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This titanic_titanic_titanic_data_clean_clean_cleanset can be found at https://www.kaggle.com/competitions/spaceship-titanic \n",
    "***\n",
    "#### titanic_data_clean Fields:\n",
    "__PassengerId__ <br>\n",
    "A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.<br>\n",
    "__HomePlanet__ <br>\n",
    "The planet the passenger departed from, typically their planet of permanent residence.<br>\n",
    "__CryoSleep__ <br>\n",
    "Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.<br>\n",
    "__Cabin__ <br>\n",
    "The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.<br>\n",
    "__Destination__ <br>\n",
    "The planet the passenger will be debarking to.<br>\n",
    "__Age__ <br>\n",
    "The age of the passenger.<br>\n",
    "__VIP__ <br>\n",
    "Whether the passenger has paid for special VIP service during the voyage.<br>\n",
    "__RoomService, FoodCourt, ShoppingMall, Spa, VRDeck__ <br>\n",
    "Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.<br>\n",
    "__Name__ <br>\n",
    "The first and last names of the passenger.<br>\n",
    "__Transported__ <br>\n",
    "Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanic_data = pd.read_csv(\"/home/mmmarinov/ProjectPortfolio/1. Titanic_SpaceShip - Binary Classification/Titanic_SpaceShip_Train_Data.csv\")\n",
    "\n",
    "#titanic_data = pd.read_csv(r\"C:\\Users\\N179960\\OneDrive - Munich Re\\Martin Stuff\\Personal\\ProjectPortfolio\\1. Data Science\\1. Titanic_SpaceShip - Binary Classification\\Titanic_SpaceShip_Train_Data.csv\")\n",
    "\n",
    "#titanic_data = pd.read_csv(r\"\\\\192.168.68.200\\ProjectPortfolio\\1. Data Science\\1. Titanic_SpaceShip - Binary Classification\\Titanic_SpaceShip_Train_Data.csv\")\n",
    "\n",
    "titanic_data = pd.read_csv(\"/Users/martinmarinov/ProjectPortfolio/ProjectPortfolio/1. Data Science/1. Titanic_SpaceShip - Binary Classification/Titanic_SpaceShip_Train_Data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Investigate the titanic_data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 High-level understanding\n",
    "This segment is to get a general sense of what kind of information is held"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.info()\n",
    "\n",
    "# Looking at what data types the dataset hold. Which needs to be changeed to float, and which whill need to be One-Hot Encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.describe(include='all')\n",
    "\n",
    "# Viewing the mean, std, min, max for numerical values and unique for catagorical values\n",
    "# Looking for an initial understanding of the spread and common values found in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum treats the True as 1 and False as 0, Count will add everything regardless of T/F\n",
    "true_count = titanic_data[\"Transported\"].sum()\n",
    "false_count = titanic_data[\"Transported\"].count() - true_count\n",
    "\n",
    "# Data to plot\n",
    "labels = 'True', 'False'\n",
    "sizes = [true_count, false_count]\n",
    "colors = ['lightcoral', 'lightskyblue']\n",
    "explode = (0.1, 0)  # Explode the first slice (True) for emphasis\n",
    "\n",
    "# {:1f}% will return a percentage with 1 decimal point. {:d} returns the full number.\n",
    "def autopct_format(pct):\n",
    "    return \"{:.1f}%\\n({:d})\".format(pct, int(round(pct * sum(sizes) / 100)))\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct=autopct_format, shadow=True, startangle=140)\n",
    "\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "# Give a title to the chart\n",
    "plt.title(\"Transported Ratio\")\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n",
    "\n",
    "# This data tells me that there isn't a class imbalance \n",
    "# and therefore we do not need to worry about undersampling, SMOTE, or class weight approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.isna().sum()\n",
    "\n",
    "# Each column has missing information. Since it's numerical and catagorical, likely I'll need to use a mix of techniques to populate the values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical: <br>\n",
    "- Passenger_Id (If seperated) \n",
    "- Age (#)\n",
    "- RoomService ($)\n",
    "- FoodCourt ($)\n",
    "- ShoppingMall ($)\n",
    "- Spa ($)\n",
    "- VRDeck ($)\n",
    "\n",
    "Catagorical: <br>\n",
    "- HomePlant (Unique: 3)\n",
    "- Cryosleep (T/F)\n",
    "- Cabin (Deck and Side)\n",
    "- Destination (Unique: 3)\n",
    "- VIP (T/F)\n",
    "- Name (Mostly Unique)\n",
    "\n",
    "Target Variable: <br>\n",
    "- Transported (T/F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Explore the titanic_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy so that the original remains untouched\n",
    "titanic_data_eda = titanic_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking up Cabin to understand if there are trends with deck level or side of ship\n",
    "titanic_data_eda[['Deck', 'Cabin_num', 'Side']\n",
    "                 ] = titanic_data_eda['Cabin'].str.split('/', expand=True)\n",
    "\n",
    "# Breaking up Passenger to view grouping\n",
    "titanic_data_eda[['Pass_group', 'Pass_id']\n",
    "                 ] = titanic_data_eda['PassengerId'].str.split('_', expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding to view correlation, it can be interesting to see if these columns have strong relation to the target variable (Transported)\n",
    "titanic_data_eda_dummies = pd.get_dummies(\n",
    "    titanic_data_eda, columns=['Deck', 'Side'])\n",
    "\n",
    "# Since these planets and destinations are closer/farther from one another with distance, it would be best to encode them as ordinal data rather than nominal\n",
    "# The same logic can be applied to the Deck, but since we do not not for sure which is closer or farther, then we can not assign them just on alaphabetically order alone\n",
    "titanic_data_eda_dummies['HomePlanet'] = titanic_data_eda_dummies['HomePlanet'].astype(\n",
    "    'category').cat.codes\n",
    "titanic_data_eda_dummies['Destination'] = titanic_data_eda_dummies['Destination'].astype(\n",
    "    'category').cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_eda_dummies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unneeded columns. This is due to the unlikelyhood of them being valuable predictors.\n",
    "titanic_data_eda_dropped = titanic_data_eda_dummies.drop(\n",
    "    ['PassengerId', 'Cabin', 'Name', 'Cabin_num', 'Pass_id'], axis=1)\n",
    "titanic_data_eda_dropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the catagorical fields into numerical so that it can all be analyzed\n",
    "titanic_data_eda_dropped[['CryoSleep', 'VIP', 'Transported', 'Pass_group']] = titanic_data_eda_dropped[[\n",
    "    'CryoSleep', 'VIP', 'Transported', 'Pass_group']].astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_eda_dropped.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 8))\n",
    "sns.set_theme(style=\"white\")\n",
    "corr = titanic_data_eda_dropped.corr()\n",
    "heatmap = sns.heatmap(corr, annot=True, cmap=\"Blues\", fmt='.1g')\n",
    "\n",
    "# The interesting relation to note is Cryosleep. Which looks to be at 0.5 in relation with Transported without any data cleaning.\n",
    "# This looks like it is going to be a strong predictor and will need special attention when populating nulls.\n",
    "# The next highest/lowest is +-0.2 which mainly comes from the different spend categories like RoomService or Spa/VRDeck.\n",
    "# Since their relation is negative I should see that spending more decreased the chance of being transported\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['CryoSleep', 'Age', 'VIP', 'RoomService',\n",
    "           'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "sns.pairplot(titanic_data_eda_dropped, vars=columns, hue='Transported')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Blue = Transported\n",
    "\n",
    "Things to note:\n",
    "- Someone in CryoSleep does not spend any money. I can use this to populate missing CryoSleep and the Expense columns\n",
    "- The ratio of those who were transported vs not is apparent for CryoSleep and Age. Showing that these can be valuable features for prediction\n",
    "- Looking at the spending, people who spent more money at the Spa/VRdeck/RoomService were less likely to be transported. While the opposite is true for Foodcourt and Shopping Mall\n",
    "    - This ties back to the heatmap and provides a better look into the spread of the data points\n",
    "- There is an age minmium to be a VIP. This can be used to fill in null values for Age or vise versa. We know that a 5 year old can not be a VIP\n",
    "- There is an Age minimum to spending money as well. If VIP is null or False, then we can use moeny spent as a group to identify the mean value to fill in.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This proves my first obeservation with the relation to CryoSleep & Expenses\n",
    "titanic_data_eda_dropped[['CryoSleep', 'RoomService', 'FoodCourt',\n",
    "                          'Spa', 'ShoppingMall', 'VRDeck']].groupby('CryoSleep').sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The youngest VIP is 18 Year Old\n",
    "titanic_data_eda_dropped[['VIP', 'Age']].groupby('VIP').min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The youngest to spend any money is 13 Years Old\n",
    "titanic_data_eda_dropped[['RoomService', 'FoodCourt', 'Spa', 'ShoppingMall', 'VRDeck', 'Age']]\\\n",
    "    .groupby('Age')\\\n",
    "    .sum()\\\n",
    "    .head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_eda_dropped.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['HomePlanet', 'Destination', 'Pass_group', 'Deck_A', 'Deck_B', 'Deck_C', 'Deck_D',\n",
    "           'Deck_E', 'Deck_F', 'Deck_G', 'Deck_T', 'Side_P', 'Side_S']\n",
    "sns.pairplot(titanic_data_eda_dropped, vars=columns, hue='Transported')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Blue = Transported\n",
    "\n",
    "Things to note:\n",
    "- HomePlanet has some ability to predict the target variable. This ties back well to the heatmap. \n",
    "- Deck Side as expected is redudent. Keeping 1 would be fine. \n",
    "- Deck Level do not show much valuable information. It would be best to test whether the model fits better with or without this information.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_group_counts = titanic_data_eda_dropped['Pass_group'].value_counts()\n",
    "pass_group_counts_greater_than_one = pass_group_counts[pass_group_counts > 1]\n",
    "pass_group_counts_greater_than_one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if I can use the passenger group to fill in null values\n",
    "titanic_data_eda_dropped[titanic_data_eda_dropped['Pass_group'].isin(pass_group_counts_greater_than_one.index) &\n",
    "                         (titanic_data_eda_dropped['VIP'].isna())]\n",
    "\n",
    "\n",
    "# (titanic_data_eda_dropped['VIP'].isna())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Clean the titanic_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del titanic_data_clean\n",
    "del titanic_data_clean_complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the titanic_data_clean once more to start fresh and seperate out the eda\n",
    "titanic_data_clean = titanic_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_clean.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What fields to use: <br>\n",
    ">PassengerId  | Exclude  <br>\n",
    "HomePlanet   | Include <br>\n",
    "CryoSleep    | Include  <br>\n",
    "Cabin        | T/E Deck, Keep 1 Side  <br> \n",
    "Destination  | Include  <br>\n",
    "Age          | Include  <br>\n",
    "VIP          | Include  <br>\n",
    "RoomService  | Include  <br>\n",
    "FoodCourt    | Include  <br>\n",
    "ShoppingMall | Include  <br>\n",
    "Spa          | Include  <br>\n",
    "VRDeck       | Include  <br>\n",
    "Name         | Exclude  <br>\n",
    "\n",
    "What I know regarding populating missing values\n",
    "- Those who are in CryoSleep could not have spent any money, therefore if CryoSleep = True then the null expenses values are 0\n",
    "    - If Cryosleep is null, then sum the expenses and if it is 0 then set the value to True\n",
    "    - If Cryosleep is True and an expense is null, then likely taking the mean of that column will be sufficent\n",
    "- If Age is null, then I can use VIP and whether they spent money or not as a way to identify.\n",
    "    - People under the age of 18 look like they can't be VIP based on the PairPlot. So if a null Age is VIP then we can use the mean of the VIP group to fill in the null value. \n",
    "    - Same goes for money spent. If there is any money spent, then we know they are at least 13 years or older meaning we can derive the mean age from that group. \n",
    "- I can use the passenger group to fill in missing home planet or Deck Side values, otherwise take the most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_clean.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populating NAN's using Domain Logic & Catagorical Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = titanic_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cryosleep & Expenses\n",
    "# sum all $ columns into one expense column\n",
    "expense_columns = ['RoomService', 'FoodCourt',\n",
    "                    'ShoppingMall', 'Spa', 'VRDeck']\n",
    "# add this column to the dataframe\n",
    "df['Expenses'] = df[expense_columns].sum(axis=1)\n",
    "# run a script that populates null CryoSleep based on whether or not the expense column has $0 or not\n",
    "df['CryoSleep'] = df.apply(lambda row: True if pd.isna(\n",
    "    row['CryoSleep']) and row['Expenses'] == 0 else False, axis=1).astype('bool')\n",
    "\n",
    "# Populates NaN Expenses based on whether the person is in CryoSleep or not. \n",
    "# True Cryosleep means they couldn't have spent money.\n",
    "for column in expense_columns:\n",
    "    df[column] = df.apply(lambda row: 0 if pd.isna(\n",
    "        row[column]) and row['CryoSleep'] == True else row[column], axis=1).astype('float64')\n",
    "    df[column] = df.apply(lambda row: df[column].mean() if pd.isna(\n",
    "        row[column]) else row[column], axis=1).astype('float64')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['CryoSleep']==True][expense_columns].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age & VIP\n",
    "df['VIP'] = df.apply(lambda row: False\n",
    "                        if pd.isna(row['VIP']) and row['Age'] < 18\n",
    "                        else row['VIP'], axis=1).astype('bool')\n",
    "\n",
    "VIP_true = df[df['VIP'] == True].Age.mean()\n",
    "VIP_false = df[df['VIP'] == False].Age.mean()\n",
    "\n",
    "df['Age'] = df.apply(lambda row: VIP_true\n",
    "                        if pd.isna(row['Age']) and row['VIP'] == True\n",
    "                        else row['Age'], axis=1).astype('float64')\n",
    "df['Age'] = df.apply(lambda row: VIP_false\n",
    "                        if pd.isna(row['Age']) and row['VIP'] == False\n",
    "                        else row['Age'], axis=1).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Age','VIP']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Age for VIP's: {VIP_true:.1f} Yrs \\nMean Age for Non-VIP's: {VIP_false:.1f} Yrs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catagorical Imputing\n",
    "\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "categorical_columns = ['HomePlanet', 'Destination','Cabin','VIP']\n",
    "\n",
    "# Impute missing values in the categorical columns\n",
    "df[categorical_columns] = categorical_imputer.fit_transform(df[categorical_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Deck', 'Cabin_num', 'Side']] = df['Cabin'].str.split('/', expand=True)\n",
    "df[['Pass_group', 'Pass_id']] = df['PassengerId'].str.split('_', expand=True)\n",
    "\n",
    "df['Pass_group'] = pd.to_numeric(df['Pass_group'])\n",
    "\n",
    "df = pd.get_dummies(df, columns=['Deck', 'Side'])\n",
    "\n",
    "df['HomePlanet'] = df['HomePlanet'].astype('category').cat.codes\n",
    "df['Destination'] = df['Destination'].astype('category').cat.codes\n",
    "\n",
    "df = df.drop(['PassengerId', 'Cabin', 'Name',\n",
    "                'Cabin_num', 'Pass_id', 'Side_S'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Pipeline & Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Identify Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder,OneHotEncoder\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data = pd.read_csv(\"/workspaces/ProjectPortfolio/1. Data Science/1. Titanic_SpaceShip - Binary Classification/Titanic_SpaceShip_Train_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CryoExpenseImputer(df):\n",
    "    expense_columns = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    df['Expenses'] = df[expense_columns].sum(axis=1)\n",
    "    df['CryoSleep'] = df.apply(lambda row: True if pd.isna(row['CryoSleep']) and row['Expenses'] == 0 else False, axis=1).astype('float64')\n",
    "    for column in expense_columns:\n",
    "        df[column] = df.apply(lambda row: 0 if pd.isna(row[column]) and row['CryoSleep'] == True else row[column], axis=1).astype('float64')\n",
    "        df[column] = df.apply(lambda row: df[column].mean() if pd.isna(row[column]) else row[column], axis=1).astype('float64')\n",
    "    return df\n",
    "\n",
    "def VIPAgeImputer(df):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    si = SimpleImputer(strategy='most_frequent')\n",
    "    df['VIP'] = df.apply(lambda row: False if pd.isna(row['VIP']) and row['Age'] < 18 else row['VIP'], axis=1).astype('float64')\n",
    "    df['VIP'] = si.fit_transform(df['VIP'].array.reshape(-1,1))\n",
    "    \n",
    "    VIP_true = df[df['VIP'] == True].Age.mean()\n",
    "    VIP_false = df[df['VIP'] == False].Age.mean()\n",
    "\n",
    "    df['Age'] = df.apply(lambda row: VIP_true if pd.isna(row['Age']) and row['VIP'] == True else row['Age'], axis=1).astype('float64')\n",
    "    df['Age'] = df.apply(lambda row: VIP_false if pd.isna(row['Age']) and row['VIP'] == False else row['Age'], axis=1).astype('float64')  \n",
    "    return df\n",
    "\n",
    "def HomeDestImputer(df):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "    si = SimpleImputer(strategy='most_frequent')\n",
    "    oe = OrdinalEncoder()\n",
    "\n",
    "    home_dest_cols = ['HomePlanet', 'Destination']\n",
    "\n",
    "    for column in home_dest_cols:\n",
    "        imputed_data = si.fit_transform(df[column].array.reshape(-1, 1))\n",
    "        df[column] = imputed_data.ravel()  \n",
    "        df[column] = oe.fit_transform(df[column].array.reshape(-1, 1))\n",
    "\n",
    "    return df  \n",
    "\n",
    "def CatagoryTransform(df):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    si = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    imputed_data = si.fit_transform(df['Cabin'].array.reshape(-1, 1))\n",
    "    df['Cabin'] = imputed_data.ravel() \n",
    "    df[['Deck', 'Cabin_num', 'Side']] = df['Cabin'].str.split('/', expand=True)\n",
    "\n",
    "    categorical_columns = ['Deck', 'Side']\n",
    "\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', drop='if_binary', sparse_output=False)\n",
    "    encoder.fit(df[categorical_columns])\n",
    "\n",
    "    df_encoded_columns = encoder.transform(df[categorical_columns])\n",
    "\n",
    "    df_encoded = pd.DataFrame(df_encoded_columns, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "    df = pd.concat([df.drop(categorical_columns, axis=1).reset_index(drop=True), df_encoded], axis='columns')\n",
    "\n",
    "    return df\n",
    "\n",
    "def DropColumns(df):\n",
    "    df = df.drop(['PassengerId','Cabin','Name','Cabin_num'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    {\n",
    "        'name': 'Logistic Regression',\n",
    "        'classifier': LogisticRegression(solver='lbfgs', max_iter=10000),\n",
    "        'params': {\n",
    "            'C': [0.1, 1.0, 10.0]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'classifier': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'max_depth': [None, 10, 20]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Gradient Boosting',\n",
    "        'classifier': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'learning_rate': [0.1, 0.01]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'XGBoost',\n",
    "        'classifier': xgb.XGBClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.1, 0.01]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic_data.drop(['Transported'], axis=1)\n",
    "y = titanic_data['Transported']\n",
    "\n",
    "function_list = [CryoExpenseImputer,VIPAgeImputer,HomeDestImputer,CatagoryTransform,DropColumns]    \n",
    "\n",
    "for function in function_list:\n",
    "    X  = function(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for classifier_info in classifiers:\n",
    "    classifier = classifier_info['classifier']\n",
    "    params = classifier_info['params']\n",
    "\n",
    "    grid_search = GridSearchCV(classifier, param_grid=params, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    if grid_search.best_score_ > best_accuracy:\n",
    "        best_accuracy = grid_search.best_score_ \n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_model_name = classifier_info['name']\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model.fit(X, y)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Training Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
